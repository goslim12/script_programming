{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 id=\"[14&#48264;-&#47928;&#51228;]\">[14&#48264; &#47928;&#51228;]<a class=\"anchor-link\" href=\"#[14&#48264;-&#47928;&#51228;]\">&#182;</a></h5><p>이전 Assignment 3의 마지막 문제는 웹 URL로 지정된 웹페이지를 문자열로 가져와 모든 HTML 태그 및 CSS와 Javascript를 제외한 순수 텍스트를 얻어내고 그 안에 존재하는 단어를 추출하여 각 단어들에 대해 출현빈도를 사전형태({'world': 2, 'hello': 1, 'python': 1})로 저장하여 출력하는 것이었다. 이번에는 Assignment 3를 확장하여 다음과 같은 조건을 만족하도록 구현하시오.</p>\n",
    "<ul>\n",
    "<li>1) 다음 사이트에서 제시되는 영어 불용어를 참고하여 이전 숙제에서 구성했던 단어 사전에서 영어 불용어들을 모두 제거하는 코드를 추가하시오.<ul>\n",
    "<li><a href=\"http://egloos.zum.com/wyb330/v/3029348\">http://egloos.zum.com/wyb330/v/3029348</a></li>\n",
    "<li>영어 불용어: [ 'a', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it', 'no', 'not', 'of', 'on', 'or', 's', 'such', 't', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with']</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>2) 각 URL로 지정된 웹페이지의 HTML 소스를 파일로 저장하시오. <ul>\n",
    "<li>URL이 <a href=\"http://URL\">http://URL</a> 이라면 파일명은 URL.html 이다. </li>\n",
    "<li>예: URL이 <a href=\"http://www.cnn.com\">http://www.cnn.com</a> 이라면 파일명은 www.cnn.com.html 이다. </li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>3) 단어의 출현빈도가 담긴 사전 객체를 위 HTML 소스 파일과 동일한 폴더에 파일로 저장하시오.<ul>\n",
    "<li>파일입출력 (E-learning 13주차) 마지막에 학습한 pickle 모듈을 활용하시오. </li>\n",
    "<li>URL이 <a href=\"http://URL\">http://URL</a> 이라면 사전 객체를 담고 있는 파일명은 URL.words.frequency 이다.</li>\n",
    "<li>예: URL이 <a href=\"http://www.cnn.com\">http://www.cnn.com</a> 이라면 파일명은 www.cnn.com.words.frequency 이다.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>4) 최소 5개 이상의 영어 웹 사이트에 대한 HTML 소스 파일과 단어 출현빈도 파일을 저장하시오. <ul>\n",
    "<li>즉, 총 10개의 파일을 동일한 폴더에 생성하시오.</li>\n",
    "<li>[주의] 영어 웹사이트 URL로만 5개 이상</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>5) 위 문제에서 저장한 모든 words.frequency 파일들을 객체로 다시 로드하여 본인이 저장하여 분석한 사이트들에서 가장 많이 출현한 단어 3개를 뽑아 제시하시오. <ul>\n",
    "<li>반드시 pickle 모듈로 저장한 5개 이상의 words.frequency를 다시 5개 이상의 사전 객체로 로드 하는 코드가 추가되어야 함</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "def tag_delete(string):\n",
    "    for i in range(0, string.count('<!--')):\n",
    "        if string.find(\"<!--\") != -1:\n",
    "            string = string.replace(string[string.find(\"<!--\"):string.find(\"-->\")+3:],\" \")\n",
    "    for i in range(0,string.count(\"<\")):\n",
    "        if string.find(\"<\") != -1:\n",
    "            string = string.replace(string[string.find(\"<\"):string.find(\">\")+1:],\" \")\n",
    "    \n",
    "    return string\n",
    "\n",
    "#자바스크립트 제거\n",
    "def javascript_delete(string):\n",
    "    for i in range(0, string.count('<script')):\n",
    "        if string.find(\"<script\") != -1:\n",
    "            string = string.replace(string[string.find(\"<script\"):string.find(\"</script>\")+9:],\" \")\n",
    "    return string\n",
    "\n",
    "#css 제거\n",
    "def stylesheet_delete(string):\n",
    "    for i in range(0, string.count('<style')):\n",
    "        if string.find(\"<style\") != -1:\n",
    "            string = string.replace(string[string.find(\"<style\"):string.find(\"</style>\")+8:],\" \")\n",
    "    return string\n",
    "\n",
    "#구두문자 제거 함수\n",
    "def punc_delete(source):\n",
    "    punctuation = string.punctuation\n",
    "    for i in range (len(punctuation)):\n",
    "        source= source.replace(punctuation[i],\" \")\n",
    "        \n",
    "    return source\n",
    "\n",
    "#불용어 제거 함수\n",
    "def stopword_delete(words):\n",
    "    stopword = [ 'A','a', 'and', 'are', 'as', 'at', 'be', 'but','But', 'by', 'for', 'if','If', 'in', 'into', 'is', 'it','It', 'no','No', 'not','Not', 'of', 'on', 'or', 's', 'such', 't', 'that','That', 'the','The', 'their','Their', 'then','Then', 'there','There', 'these','These', 'they','They', 'this','This', 'to', 'was', 'will', 'with']\n",
    "    for i in range (len(stopword)):\n",
    "        for j in range(len(words)):\n",
    "            if words[j] == stopword[i]:\n",
    "                words[j] = \" \"\n",
    "\n",
    "\n",
    "#사전형태로 출현빈도 저장 함수\n",
    "def word_count(words):\n",
    "    words_dict = {}\n",
    "    for i in words:\n",
    "        if words_dict.has_key(i) is False:\n",
    "            words_dict[i]=1\n",
    "        else:\n",
    "            words_dict[i]+=1\n",
    "    return words_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "f = open('www.cnn.com.html','w')\n",
    "f.write(urllib2.urlopen(\"http://www.cnn.com\").read()) # 문자열을 파일에 기록\n",
    "f.close()\n",
    "\n",
    "f = file('www.cnn.com.html') # f = open('t.txt', 'r')과 동일\n",
    "source = f.read()\n",
    "f.close()\n",
    "\n",
    "source = stylesheet_delete(source)\n",
    "source = javascript_delete(source)\n",
    "source = tag_delete(source)\n",
    "source = punc_delete(source)\n",
    "words = source.split()  \n",
    "stopword_delete(words)\n",
    "source = \" \".join(words)\n",
    "words = source.split()\n",
    "words = word_count(words)\n",
    "\n",
    "f = open('www.cnn.com.words.frequency.txt', 'w')\n",
    "pickle.dump(words, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "f = open('www.about.com.html','w')\n",
    "f.write(urllib2.urlopen(\"http://www.about.com\").read()) # 문자열을 파일에 기록\n",
    "f.close()\n",
    "\n",
    "f = file('www.about.com.html') # f = open('t.txt', 'r')과 동일\n",
    "source = f.read()\n",
    "f.close()\n",
    "\n",
    "source = stylesheet_delete(source)\n",
    "source = javascript_delete(source)\n",
    "source = tag_delete(source)\n",
    "source = punc_delete(source)\n",
    "words = source.split()  \n",
    "stopword_delete(words)\n",
    "source = \" \".join(words)\n",
    "words = source.split()\n",
    "words = word_count(words)\n",
    "\n",
    "f = open('www.about.com.words.frequency.txt', 'w')\n",
    "pickle.dump(words, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "f = open('www.time.com.html','w')\n",
    "f.write(urllib2.urlopen(\"http://www.time.com\").read()) # 문자열을 파일에 기록\n",
    "f.close()\n",
    "\n",
    "f = file('www.time.com.html') # f = open('t.txt', 'r')과 동일\n",
    "source = f.read()\n",
    "f.close()\n",
    "\n",
    "source = stylesheet_delete(source)\n",
    "source = javascript_delete(source)\n",
    "source = tag_delete(source)\n",
    "source = punc_delete(source)\n",
    "words = source.split()  \n",
    "stopword_delete(words)\n",
    "source = \" \".join(words)\n",
    "words = source.split()\n",
    "words = word_count(words)\n",
    "\n",
    "f = open('www.time.com.words.frequency.txt', 'w')\n",
    "pickle.dump(words, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "f = open('www.sciencemag.org.html','w')\n",
    "f.write(urllib2.urlopen(\"http://www.sciencemag.org\").read()) # 문자열을 파일에 기록\n",
    "f.close()\n",
    "\n",
    "f = file('www.sciencemag.org.html') # f = open('t.txt', 'r')과 동일\n",
    "source = f.read()\n",
    "f.close()\n",
    "\n",
    "source = stylesheet_delete(source)\n",
    "source = javascript_delete(source)\n",
    "source = tag_delete(source)\n",
    "source = punc_delete(source)\n",
    "words = source.split()  \n",
    "stopword_delete(words)\n",
    "source = \" \".join(words)\n",
    "words = source.split()\n",
    "words = word_count(words)\n",
    "\n",
    "f = open('www.sciencemag.org.words.frequency.txt', 'w')\n",
    "pickle.dump(words, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "f = open('www.bbc.co.uk.html','w')\n",
    "f.write(urllib2.urlopen(\"http://www.bbc.co.uk\").read()) # 문자열을 파일에 기록\n",
    "f.close()\n",
    "\n",
    "f = file('www.bbc.co.uk.html') # f = open('t.txt', 'r')과 동일\n",
    "source = f.read()\n",
    "f.close()\n",
    "\n",
    "source = stylesheet_delete(source)\n",
    "source = javascript_delete(source)\n",
    "source = tag_delete(source)\n",
    "source = punc_delete(source)\n",
    "words = source.split()  \n",
    "stopword_delete(words)\n",
    "source = \" \".join(words)\n",
    "words = source.split()\n",
    "words = word_count(words)\n",
    "\n",
    "f = open('www.bbc.co.uk.words.frequency.txt', 'w')\n",
    "pickle.dump(words, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('www.bbc.co.uk.words.frequency.txt')\n",
    "bbc = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('www.sciencemag.org.words.frequency.txt')\n",
    "sciencemag = pickle.load(f) # 인스턴스 가져오기\n",
    "f.close()\n",
    "\n",
    "f = open('www.time.com.words.frequency.txt')\n",
    "time = pickle.load(f) # 인스턴스 가져오기\n",
    "f.close()\n",
    "\n",
    "f = open('www.about.com.words.frequency.txt')\n",
    "about = pickle.load(f) # 인스턴스 가져오기\n",
    "f.close()\n",
    "\n",
    "f = open('www.cnn.com.words.frequency.txt')\n",
    "cnn = pickle.load(f) # 인스턴스 가져오기\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combine ={}\n",
    "for i in bbc:\n",
    "    combine[i] = bbc[i]\n",
    "    \n",
    "for i in sciencemag:\n",
    "    if combine.has_key(i):\n",
    "        combine[i] += sciencemag[i]\n",
    "    else:\n",
    "        combine[i] = sciencemag[i]\n",
    "\n",
    "for i in time:\n",
    "    if combine.has_key(i):\n",
    "        combine[i] += time[i]\n",
    "    else:\n",
    "        combine[i] = time[i]\n",
    "\n",
    "for i in about:\n",
    "    if combine.has_key(i):\n",
    "        combine[i] += about[i]\n",
    "    else:\n",
    "        combine[i] = about[i]\n",
    "        \n",
    "for i in cnn:\n",
    "    if combine.has_key(i):\n",
    "        combine[i] += cnn[i]\n",
    "    else:\n",
    "        combine[i] = cnn[i]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Travel', 86)\n",
      "('Food', 48)\n",
      "('BBC', 38)\n"
     ]
    }
   ],
   "source": [
    "g = combine.items()\n",
    "g.sort(key=lambda x:x[1],reverse=True)\n",
    "for i in range(3):\n",
    "    print g[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
